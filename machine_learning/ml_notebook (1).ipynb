{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine learning notebook for FDD Project @ Vasakronan summer internship 2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "directory = '../meter_obs_and_irr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model for the generated energy/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_energy_day_models():\n",
    "        k = 0\n",
    "        files = Path(directory).glob('**/*')\n",
    "        for filepath in files:\n",
    "                print(filepath)\n",
    "                data = pd.read_csv(filepath, skipinitialspace=True)\n",
    "                meter_id = data['metry_meter_id'][0]\n",
    "                print(\"doing id: \" + meter_id)\n",
    "                data = data.dropna(how = 'any')\n",
    "                data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "                data = data.groupby(data.date_time.dt.date).agg({\"irradiance\": \"sum\", \"generated_energy\": \"sum\"})\n",
    "                data['month'] = pd.to_datetime(\n",
    "                data.round({'irradiance': 1, 'generated_energy': 1})\n",
    "\n",
    "                x = data[[\"irradiance\", \"month\"]]\n",
    "                y = np.asarray(data[[\"generated_energy\"]])\n",
    "                if len(y) == 0:\n",
    "                        continue\n",
    "                \n",
    "                x_train, x_test, y_train, y_test = train_test_split(\n",
    "                        x, y, test_size=0.1, random_state=13)\n",
    "\n",
    "                model = RandomForestRegressor(max_features=\"auto\", bootstrap=True,\n",
    "                max_depth= 80,\n",
    "                min_samples_leaf= 5,\n",
    "                min_samples_split= 12,\n",
    "                n_estimators= 100)\n",
    "                model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "                print(model.score(x_test, y_test))\n",
    "\n",
    "                preds = model.predict(x_test)\n",
    "                print(len(preds))\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                        print('hej')\n",
    "                        print(preds[i])\n",
    "                        print(y_test[i])\n",
    "                        print(\"Generated div pred\", y_test[i]/preds[i])\n",
    "\n",
    "                pickle.dump(model, open(f'../per_meter_models_energy/model_{meter_id}_energy_per_day.pkl', 'wb'))\n",
    "                k += 1\n",
    "\n",
    "create_energy_day_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_effect_hour_models():\n",
    "    k = 0\n",
    "    files = Path(directory).glob('**/*')\n",
    "    done_ml = Path('./').glob('**/*.pkl')\n",
    "    model_list = []\n",
    "    for model in done_ml:\n",
    "        model_list.append(str(model))\n",
    "    for filepath in files:\n",
    "        data = pd.read_csv(filepath)\n",
    "        meter_id = data['metry_meter_id'][0]\n",
    "\n",
    "        if meter_id in ''.join(model_list):\n",
    "            continue\n",
    "        \n",
    "        print(\"doing id: \" + meter_id)\n",
    "        data = data.dropna(how = 'any')\n",
    "        #try:\n",
    "        data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "        data['month'] = data['date_time'].dt.month\n",
    "        data.round({'irradiance': 1, 'generated_energy': 1})\n",
    "        # Create a train and test set\n",
    "        x = data[[\"irradiance\", \"month\"]]\n",
    "        y = np.asarray(data[[\"generated_energy\"]])\n",
    "\n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "                x, y, test_size=0.10)\n",
    "        #    filtered_data = data.loc[(data['date_time'] >= '2021-04-01T00:00:00Z')\n",
    "        #                & (data['date_time'] < '2021-11-01T00:00:00Z')]\n",
    "        #except:\n",
    "        #     filtered_data = data\n",
    "\n",
    "        model = MLPRegressor(activation = 'tanh', alpha = 0.05, early_stopping = True, hidden_layer_sizes = (50, 100, 50), learning_rate = 'constant', max_iter = 500, solver = 'adam')\n",
    "\n",
    "        #param_grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,1)],\n",
    "        #'activation': ['relu','tanh','logistic'],\n",
    "        #'alpha': [0.0001, 0.05],\n",
    "        #'early_stopping': [True, False],\n",
    "        #'learning_rate': ['constant','adaptive'],\n",
    "        #'max_iter': [500],\n",
    "        #'solver': ['adam']\n",
    "        #}\n",
    "\n",
    "        #gsc = GridSearchCV(\n",
    "        #    model,\n",
    "        #    param_grid,\n",
    "        #    cv=5, scoring='neg_mean_squared_error', verbose=3, n_jobs=-1)\n",
    "\n",
    "        #grid_result = gsc.fit(x_train, y_train)\n",
    "\n",
    "        #best_params = grid_result.best_params_\n",
    "        #print(best_params)\n",
    "    \n",
    "        # model = RidgeCV()\n",
    "\n",
    "        # print(model.score(x_test_scaled, y_test_scaled))\n",
    "        model.fit(x_train, y_train)\n",
    "        print(model.score(x_test, y_test))\n",
    "        k = k + 1\n",
    "        pickle.dump(model, open(f'model_{meter_id}.pkl', 'wb'))\n",
    "        #pickle.dump(scalerx, open(f'scalerx_{meter_id}.pkl', 'wb'))\n",
    "        #pickle.dump(scalery, open(f'scalery_{meter_id}.pkl', 'wb'))\n",
    "        \n",
    "\n",
    "        # print(model.score(x_test, y_test))\n",
    "        # pickle.dump(model, open(f'model_{meter_id}.pkl', 'wb'))\n",
    "\n",
    "create_effect_hour_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./sthlm_csv_test2.csv')\n",
    "data = data.dropna(how = 'any')\n",
    "\n",
    "def mlp_model(X, Y):\n",
    "\n",
    "    estimator=MLPRegressor()\n",
    "\n",
    "\n",
    "    param_grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,1)],\n",
    "            'activation': ['relu','tanh','logistic'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "            'solver': ['adam']}\n",
    "\n",
    "    gsc = GridSearchCV(\n",
    "        estimator,\n",
    "        param_grid,\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "    grid_result = gsc.fit(X, Y)\n",
    "\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    best_mlp = MLPRegressor(hidden_layer_sizes = best_params[\"hidden_layer_sizes\"], \n",
    "                            activation =best_params[\"activation\"],\n",
    "                            solver=best_params[\"solver\"],\n",
    "                            max_iter= 5000, n_iter_no_change = 200\n",
    "                )\n",
    "\n",
    "    scoring = {\n",
    "            'abs_error': 'neg_mean_absolute_error',\n",
    "            'squared_error': 'neg_mean_squared_error',\n",
    "            'r2':'r2'}\n",
    "\n",
    "    scores = cross_validate(best_mlp, X, Y, cv=10, scoring=scoring, return_train_score=True, return_estimator = True)\n",
    "    return scores\n",
    "\n",
    "mlp_model(data[[\"irradiance\"]][:100],\n",
    "np.asarray(data[[\"generated_energy\"]][:100]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
